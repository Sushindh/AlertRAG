# ğŸš¨ AlertRAG â€“ LLM-assisted Incident Analysis for Cloud-Native Services

AlertRAG is an **endâ€‘toâ€‘end observability + incident intelligence platform** that detects production issues using **metrics and traces**, correlates them automatically, and generates **humanâ€‘readable AI explanations** for incidents.

This project demonstrates how modern SRE / DevOps systems can move beyond dashboards and alerts into **actionable incident understanding** using:

* OpenTelemetry
* Prometheus
* Distributed tracing
* Incident correlation
* LLMâ€‘based Root Cause Analysis (RAGâ€‘style)

---

## âœ¨ What Problem Does AlertRAG Solve?

Traditional monitoring answers:

> *â€œIs something broken?â€*

AlertRAG answers:

> *â€œWhat broke, why it broke, which requests were affected, and what should we do next?â€*

It automatically:

* Detects abnormal latency & errors
* Correlates slow traces with metrics
* Groups evidence into incidents
* Explains incidents in plain English using AI

---

## ğŸ§  Highâ€‘Level Architecture

```
[ payment-service ]
       â”‚
       â”‚ OpenTelemetry (metrics + traces)
       â–¼
[ OpenTelemetry Collector ]
       â”‚
       â”œâ”€â”€â–º Prometheus (metrics)
       â””â”€â”€â–º Trace Store (OTLP)

[ Incident Engine ]
       â”‚
       â”œâ”€â”€ PromQL (latency / error detection)
       â”œâ”€â”€ Trace correlation
       â””â”€â”€ Incident generation

[ AI Explanation Engine ]
       â”‚
       â””â”€â”€ Humanâ€‘readable RCA & mitigation steps
```

---

## ğŸ”§ Tech Stack

### Observability

* **FastAPI** â€“ Demo microservice
* **OpenTelemetry SDK** â€“ Metrics & traces
* **OpenTelemetry Collector** â€“ Telemetry pipeline
* **Prometheus** â€“ Metrics backend

### Incident Intelligence

* **PromQL** â€“ SLO / anomaly detection
* **Trace correlation** â€“ Identify slow requests
* **Python Incident Engine** â€“ Detection logic
* **LLM (Azure OpenAI / OpenAI)** â€“ AI explanations

### Infrastructure

* **Docker & Docker Compose** â€“ Local orchestration

---

## ğŸ“ Repository Structure

```
AlertRAG/
â”œâ”€â”€ client/ (Not Integrated)
â”‚  
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ incident_engine.py     # Metrics + trace correlation
â”‚   â”œâ”€â”€ ai_explainer.py        # LLM-based RCA generator
â”‚   â”œâ”€â”€ main.py                # /detect API
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ observability/
|   payment-service/
â”‚      â”œâ”€â”€ app.py              # Instrumented FastAPI service
â”‚      â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ otel-collector.yaml
â”‚   â””â”€â”€ prometheus.yml
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ How It Works (Endâ€‘toâ€‘End Flow)

### 1ï¸âƒ£ Telemetry Generation

The `payment-service` emits:

* **Histograms** â†’ request latency
* **Counters** â†’ failures
* **Traces** â†’ per-request execution

Each `/pay` request randomly sleeps (0.5s â€“ 4s) to simulate latency spikes.

---

### 2ï¸âƒ£ Metrics Collection

OpenTelemetry Collector exports metrics to Prometheus.

Key metrics:

* `payment_latency_seconds_bucket`
* `http_server_request_count`

---

### 3ï¸âƒ£ Incident Detection

The Incident Engine periodically runs **PromQL queries**:

* **Latency (p95)**

```promql
histogram_quantile(
  0.95,
  sum(rate(payment_latency_seconds_bucket[1m])) by (le)
)
```

* **Errors**

```promql
sum(rate(http_server_request_count{http_status_code=~"5.."}[1m]))
```

If thresholds are breached â†’ an incident is created.

---

### 4ï¸âƒ£ Trace Correlation

Once an incident is detected:

* Slow traces are fetched
* Each trace includes:

  * `trace_id`
  * latency
  * endpoint
  * timestamp

This links **"numbers on dashboards"** to **real user requests**.

---

### 5ï¸âƒ£ AIâ€‘Powered Explanation

The incident data is passed to an LLM which generates:

* What happened
* Likely root cause
* Immediate mitigation steps
* Longâ€‘term preventive actions

Example output:

```json
{
  "type": "High Latency",
  "latency_p95": "4.75",
  "slow_traces": [...],
  "ai_explanation": "The payment-service experienced..."
}
```

---

## ğŸ“¡ API Endpoints

### Detect Incidents

```
POST /detect
```

Response:

```json
{
  "incidents": [
    {
      "id": "INC-1",
      "service": "payment-service",
      "type": "High Latency",
      "latency_p95": "4.75",
      "errors": "4.35",
      "slow_traces": [...],
      "ai_explanation": "..."
    }
  ]
}
```

---

## â–¶ï¸ Running Locally

### Prerequisites

* Docker
* Docker Compose

### /server/env
AZURE_OPENAI_ENDPOINT= GET IT FROM AZURE
AZURE_OPENAI_KEY= GET IT FROM AZURE
AZURE_OPENAI_DEPLOYMENT= GET IT FROM AZURE
AZURE_OPENAI_API_VERSION= GET IT FROM AZURE

### Start Everything

```
docker-compose up --build
```

### Generate Load

```
for i in {1..30}; do curl http://localhost:8000/pay; done
```

### Detect Incidents

```
curl -X POST http://localhost:9000/detect
```

---

## ğŸ§ª Demo Scenario

* Latency spikes above 3â€“4 seconds
* p95 latency crosses threshold
* Incident is created
* Slow traces identified
* AI explains the outage

This mimics **real production incidents**.

---

## ğŸŒŸ Why This Project Is Interesting

* Combines **metrics + traces + AI** (rare in demos)
* Shows real SRE workflows
* Not just alerting â€” **incident understanding**
* Clean separation of observability & intelligence

---

## ğŸ›£ï¸ Future Works:

* **Clientâ€“Server Integration**: Complete integration between the frontend client and backend incident engine for real-time incident visualization.
* **Vector Databaseâ€“Backed Incident Memory**: Store historical incidents and AI-generated explanations in an **Azure Vector Database**, enabling retrieval-augmented generation (RAG) with **Azure OpenAI** to improve accuracy, consistency, and contextual understanding over time.
* **AlertRAG as a Managed Service**: Deploy AlertRAG as a scalable platform/service so that any application can onboard by simply sending OpenTelemetry data and consume AI-powered incident intelligence.

---

## ğŸ‘¤ Author

**Sushindh Anandan**
B.Tech CSE at VIT, Chennai

##
